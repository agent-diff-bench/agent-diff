{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy pandas matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LuL2fQDHAvB0",
   "metadata": {
    "id": "LuL2fQDHAvB0"
   },
   "source": [
    "# Import Libraries and Files for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqFlkWG0CnP2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 1877,
     "status": "ok",
     "timestamp": 1768571025249,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "vqFlkWG0CnP2",
    "outputId": "6538ab4b-5b65-464b-82cd-e8a16cba8113"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Expected format of merged results (flat list, one entry per test run):\n",
    "[\n",
    "    {\n",
    "        \"prompt\": str,                  # Task prompt given to agent\n",
    "        \"status\": str,                  # \"passed\" or \"failed\"\n",
    "        \"passed\": bool,                 # True if test passed\n",
    "        \"score\": float,                 # Score (0-100)\n",
    "        \"time\": float,                  # Execution time in seconds\n",
    "        \"failures\": list[str],          # List of failure messages (empty if passed)\n",
    "        \"runId\": str,                   # Unique run identifier\n",
    "        \"model\": str,                   # Model name (e.g., \"openai/gpt-5-mini\")\n",
    "        \"test_id\": str,                 # Test case UUID\n",
    "        \"test_name\": str,               # Human-readable test name\n",
    "        \"service\": str,                 # API service (e.g., \"slack\", \"box\", \"calendar\")\n",
    "        \"test_suite_name\": str,         # Test suite name\n",
    "        \"include_api_docs\": bool,       # Whether API docs were included in prompt\n",
    "        \"timestamp\": str,               # ISO timestamp of run\n",
    "        \"trace\": {                      # Agent execution trace\n",
    "            \"steps\": [                  # List of agent iterations\n",
    "                {\n",
    "                    \"iteration\": int,\n",
    "                    \"thinking\": str,\n",
    "                    \"action\": str,\n",
    "                    \"observation\": {\"stdout\": str, \"stderr\": str, \"exit_code\": int},\n",
    "                    \"usage\": {\"prompt_tokens\": int, \"completion_tokens\": int, \"total_tokens\": int, \"cost\": float}\n",
    "                },\n",
    "                ...\n",
    "            ],\n",
    "            \"final\": {...} | None,      # Final step (with summary) or None if incomplete\n",
    "            \"iterations\": int,          # Total iterations\n",
    "            \"completed\": bool,          # Whether agent completed (vs timeout/max iterations)\n",
    "            \"usage\": {...}              # Aggregated token usage and cost\n",
    "        },\n",
    "        \"diff\": {                       # Database state changes\n",
    "            \"inserts\": [...],           # New records created\n",
    "            \"updates\": [...],           # Records modified\n",
    "            \"deletes\": [...]            # Records deleted\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "# Load checkpoint files (new flat format)\n",
    "checkpoint_dir = Path(\"evaluation_outputs/checkpoints\")\n",
    "\n",
    "# Load a specific checkpoint\n",
    "checkpoint_path = checkpoint_dir / \"checkpoint_20260130_183234.json\"\n",
    "with checkpoint_path.open() as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# The data is a list of completed test results (flat format)\n",
    "print(f\"Loaded {len(data)} test runs\")\n",
    "\n",
    "'''\n",
    "\n",
    "# Or load and merge multiple checkpoints\n",
    "def load_all_checkpoints(checkpoint_dir, deduplicate=True):\n",
    "    \"\"\"Load and merge all checkpoint files from a directory.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    all_data = []\n",
    "    \n",
    "    for checkpoint_file in sorted(checkpoint_dir.glob(\"checkpoint_*.json\")):\n",
    "        print(f\"Loading {checkpoint_file.name}...\")\n",
    "        with checkpoint_file.open() as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            # Extract results from checkpoint structure\n",
    "            results = checkpoint_data.get(\"results\", [])\n",
    "            all_data.extend(results)\n",
    "    \n",
    "    if deduplicate:\n",
    "        # Deduplicate by runId (keep latest occurrence)\n",
    "        seen = {}\n",
    "        for run in all_data:\n",
    "            run_id = run.get(\"runId\", run.get(\"run_id\"))\n",
    "            seen[run_id] = run\n",
    "        all_data = list(seen.values())\n",
    "    \n",
    "    print(f\"Total runs loaded: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "# Load all checkpoints\n",
    "data = load_all_checkpoints(\"evaluation_outputs/checkpoints\")\n",
    "\n",
    "# Quick preview\n",
    "print(f\"\\nModels: {set(d['model'] for d in data)}\")\n",
    "print(f\"Services: {set(d.get('service', 'unknown') for d in data)}\")\n",
    "print(f\"Test suites: {set(d.get('test_suite_name', 'unknown') for d in data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x1ceg_opnV0M",
   "metadata": {
    "id": "x1ceg_opnV0M"
   },
   "source": [
    "# Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doKxLvFInUd7",
   "metadata": {
    "id": "doKxLvFInUd7"
   },
   "outputs": [],
   "source": [
    "def check_data(data):\n",
    "    \"\"\"\n",
    "    Analyze flat merged results data.\n",
    "    \n",
    "    Expected format: list of dicts, one entry per test run with fields:\n",
    "    - model, test_id, test_name, service, runId, passed, score, time, trace, etc.\n",
    "    \"\"\"\n",
    "    models = set()\n",
    "    services = set()\n",
    "    tests_per_model = defaultdict(set)  # Track unique test_ids per model\n",
    "    runs_per_model = defaultdict(int)\n",
    "    runs_per_model_and_test_id = defaultdict(int)\n",
    "    error_runs = []\n",
    "    \n",
    "    for run in data:\n",
    "        test_id = run[\"test_id\"]\n",
    "        model = run[\"model\"]\n",
    "        run_id = run.get(\"runId\", run.get(\"run_id\", \"unknown\"))\n",
    "        service = run.get(\"service\", \"unknown\")\n",
    "        \n",
    "        models.add(model)\n",
    "        services.add(service)\n",
    "        tests_per_model[model].add(test_id)\n",
    "        runs_per_model[model] += 1\n",
    "        runs_per_model_and_test_id[f\"{model}, {test_id}\"] += 1\n",
    "        \n",
    "        # Check for errors in trace\n",
    "        trace = run.get(\"trace\", {})\n",
    "        if isinstance(trace, dict) and \"error\" in trace:\n",
    "            error_runs.append(run_id)\n",
    "    \n",
    "    print(f\"Total runs: {len(data)}\")\n",
    "    print(f\"Number of models: {len(models)}\")\n",
    "    print(f\"Services: {', '.join(sorted(services))}\")\n",
    "    print(\"\\nUnique tests per model:\")\n",
    "    for model in sorted(tests_per_model.keys()):\n",
    "        print(f\"    - {model}: {len(tests_per_model[model])} tests, {runs_per_model[model]} runs\")\n",
    "    print(f\"\\nRuns per (model, test_id) - showing first 20:\")\n",
    "    for i, (model_test_id, num_runs) in enumerate(sorted(runs_per_model_and_test_id.items())):\n",
    "        if i >= 20:\n",
    "            print(f\"    ... and {len(runs_per_model_and_test_id) - 20} more\")\n",
    "            break\n",
    "        print(f\"    - {model_test_id}: {num_runs}\")\n",
    "    print(f\"\\nNumber of error runs: {len(error_runs)}\")\n",
    "    if error_runs:\n",
    "        print(f\"Runs with errors: {error_runs[:10]}{'...' if len(error_runs) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vtneRxEEhGOs",
   "metadata": {
    "id": "vtneRxEEhGOs"
   },
   "source": [
    "# ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43q66i_E8Knh",
   "metadata": {
    "collapsed": true,
    "id": "43q66i_E8Knh"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "# ==============================================================================\n",
    "# SLACK ERROR PATTERNS - For classifying tool call responses\n",
    "# ==============================================================================\n",
    "\n",
    "# https://docs.google.com/document/d/19vO1nj1RwVpiqCjw8BjTbzMCyk46KOrmDuS_HVdDpKo/edit?usp=sharing\n",
    "\n",
    "SLACK_PATTERNS = [\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Context overflow - model generated too much context\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"context_length_exceeded|maximum context length\", re.IGNORECASE), \"context_overflow\"),\n",
    "    (re.compile(r\"tokens?\\s*(?:exceed|limit)|exceeded.*tokens?\", re.IGNORECASE), \"context_overflow\"),\n",
    "    (re.compile(r\"Input tokens exceed the configured limit\", re.IGNORECASE), \"context_overflow\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Server errors (5xx) - infrastructure issues, NOT model's fault\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"(?:Status\\s*(?:Code)?:?\\s*|HTTP\\s*|Response\\s*status:?\\s*)5\\d{2}\\b\", re.IGNORECASE), \"server_error\"),\n",
    "    (re.compile(r\"\\b5\\d{2}\\s+(?:Server\\s+)?Error\\b\", re.IGNORECASE), \"server_error\"),\n",
    "    (re.compile(r\"Internal Server Error\", re.IGNORECASE), \"server_error\"),\n",
    "    # Removed \"internal_error\" from here so it falls through to api_error (ok: false)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Code errors - Python/bash code CRASHED during execution\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"Error:\\s*Error:\\s*name\\s+'[^']+'\\s+is not defined\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"\\b(?:NameError|TypeError|KeyError|AttributeError|ValueError|IndexError|ZeroDivisionError|ImportError)\\b\"), \"code_error\"),\n",
    "    (re.compile(r\"'[^']+'\\s+object has no attribute\", re.IGNORECASE), \"code_error\"),  # AttributeError variant\n",
    "    (re.compile(r\"\\bSyntaxError\\b|invalid syntax\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"Traceback \\(most recent call last\\)\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"IndentationError|TabError\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"jq:\\s*error|bash:\\s*.*error\", re.IGNORECASE), \"code_error\"),  # bash/jq errors\n",
    "    (re.compile(r\"Code execution timed out\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"command not found\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"No module named\", re.IGNORECASE), \"code_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Tool formatting errors - parsing/invocation failed\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"JSONDecodeError|Expecting value|Unterminated string\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "    (re.compile(r\"Error invoking tool\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "    (re.compile(r\"parse error|Invalid numeric literal\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "    (re.compile(r\"is not a valid tool\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # HTTP client errors (4xx) - bad request\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"(?:Status\\s*(?:Code)?:?\\s*|HTTP\\s*|Response\\s*status:?\\s*)4\\d{2}\\b\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"\\b(?:401|403)\\b.*(?:Unauthorized|Forbidden)\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"\\b4\\d{2}\\s+Client Error\\b\", re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Slack API errors - \"ok\": false responses\n",
    "    # Covers: invalid_blocks, channel_not_found, invalid_arguments, cant_invite_self,\n",
    "    #         unsupported_endpoint, invalid_name, invalid_limit, name_taken,\n",
    "    #         users_list_not_supplied, invalid_auth, no_user, user_not_found,\n",
    "    #         cant_update_message, already_in_channel, already_reacted, not_in_channel,\n",
    "    #         no_text, invalid_blocks_format, etc.\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r'\"ok\"\\s*:\\s*false', re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"'ok'\\s*:\\s*False\", re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Business logic failures\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"(?:channel|user|message|member).*?not found\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"\\b(?:channel|user|message)_not_found\\b\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"\\bcant_invite_self\\b\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"[✗❌]\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"^\\s*Failed to\\s+\", re.IGNORECASE | re.MULTILINE), \"api_error\"),\n",
    "    (re.compile(r\"Error or no messages found\", re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generic error fallback - MUST be last\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Use negative lookahead to exclude \"Error: None\" or \"Error: null\" which are often success indicators\n",
    "    (re.compile(r\"^\\s*Error:\\s+(?!None\\b|null\\b)\", re.IGNORECASE | re.MULTILINE), \"other_error\"),\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRACE-LEVEL ERROR PATTERNS (infrastructure errors, not tool responses)\n",
    "# ==============================================================================\n",
    "\n",
    "SLACK_TRACE_ERROR_PATTERNS = [\n",
    "    (re.compile(r\"Recursion limit.*reached|GRAPH_RECURSION_LIMIT\", re.IGNORECASE), \"recursion_limit\"),\n",
    "    (re.compile(r\"context_length_exceeded|maximum context length|tokens?\\s*exceed\", re.IGNORECASE), \"context_overflow\"),\n",
    "    (re.compile(r\"rate limit|quota.*exhausted|FreeTierOnly|AllocationQuota\", re.IGNORECASE), \"rate_limit\"),\n",
    "]\n",
    "\n",
    "LINEAR_PATTERNS = [\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Context overflow - model generated too much context\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"context_length_exceeded|maximum context length\", re.IGNORECASE), \"context_overflow\"),\n",
    "    (re.compile(r\"tokens?\\s*(?:exceed|limit)|exceeded.*tokens?\", re.IGNORECASE), \"context_overflow\"),\n",
    "    (re.compile(r\"Input tokens exceed the configured limit\", re.IGNORECASE), \"context_overflow\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Server errors (5xx) - infrastructure issues, NOT model's fault\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"(?:Status\\s*(?:Code)?:?\\s*|HTTP\\s*|Response\\s*status:?\\s*)5\\d{2}\\b\", re.IGNORECASE), \"server_error\"),\n",
    "    (re.compile(r\"\\b5\\d{2}\\s+(?:Server\\s+)?Error\\b\", re.IGNORECASE), \"server_error\"),\n",
    "    (re.compile(r\"Internal Server Error\", re.IGNORECASE), \"server_error\"),\n",
    "    (re.compile(r'\"error\"\\s*:\\s*\"internal_error\"', re.IGNORECASE), \"server_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Code errors - model's Python code CRASHED during execution\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"Error:\\s*Error:\\s*name\\s+'[^']+'\\s+is not defined\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"\\b(?:NameError|TypeError|KeyError|AttributeError|ValueError|IndexError|ZeroDivisionError)\\b\"), \"code_error\"),\n",
    "    (re.compile(r\"\\bSyntaxError\\b|invalid syntax\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"Traceback \\(most recent call last\\)\", re.IGNORECASE), \"code_error\"),\n",
    "    (re.compile(r\"IndentationError|TabError\", re.IGNORECASE), \"code_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Tool formatting errors - output parsing/tool invocation failed\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"JSONDecodeError|Expecting value|Unterminated string\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "    (re.compile(r\"Error invoking tool\\s+'[^']+'\\s+with\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "    (re.compile(r\"could not (?:parse|decode)|parsing error|malformed\", re.IGNORECASE), \"tool_formatting_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # GraphQL API errors - model made bad GraphQL query\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r'\"errors\"\\s*:\\s*\\[\\s*\\{'), \"api_error\"),\n",
    "    (re.compile(r\"'errors'\\s*:\\s*\\[\\s*\\{\"), \"api_error\"),\n",
    "    (re.compile(r\"Variable\\s+'[^']*'\\s+got invalid value\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"Field\\s+'[^']*'\\s+is not defined by type\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"Cannot query field\\s+'[^']*'\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"Unknown argument\\s+'[^']*'\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"argument\\s+'[^']*'\\s+(?:of type\\s+'[^']*'\\s+)?is required\", re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # HTTP client errors (4xx)\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"(?:Status\\s*(?:Code)?:?\\s*|HTTP\\s*|Response\\s*status:?\\s*)4\\d{2}\\b\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"\\b(?:401|403)\\b.*(?:Unauthorized|Forbidden)\", re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # API response failures\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r'\"ok\"\\s*:\\s*false', re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"'ok'\\s*:\\s*False\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r'\"success\"\\s*:\\s*false', re.IGNORECASE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Business logic failures\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"Issue\\s+(?:ENG-\\d+\\s+)?not found\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"User\\s+\\w+\\s+not found\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"Cannot (?:update|create|delete|assign|unassign)\\s+(?:issue|comment|label)\\s+because\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"[✗❌]\\s*\", re.IGNORECASE), \"api_error\"),\n",
    "    (re.compile(r\"^\\s*Failed to\\s+\", re.IGNORECASE | re.MULTILINE), \"api_error\"),\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generic error fallback\n",
    "    # -------------------------------------------------------------------------\n",
    "    (re.compile(r\"^\\s*Error:\\s+\", re.IGNORECASE | re.MULTILINE), \"other_error\"),\n",
    "]\n",
    "\n",
    "\n",
    "def classify_steps(steps, classifier_func):\n",
    "    \"\"\"\n",
    "    Classifies all steps in a ReAct-style trace.\n",
    "    \n",
    "    New format: each step has {iteration, thinking, action, observation, usage}\n",
    "    where observation = {stdout, stderr, exit_code}\n",
    "    \n",
    "    Returns list of:\n",
    "      - (\"tool_call\", <outcome>)  where outcome is non-failure, api_error, etc.\n",
    "    \"\"\"\n",
    "    classified = []\n",
    "    for step in steps:\n",
    "        observation = step.get(\"observation\", {})\n",
    "        # Combine stdout and stderr for classification\n",
    "        if isinstance(observation, dict):\n",
    "            response_text = observation.get(\"stdout\", \"\") + \" \" + observation.get(\"stderr\", \"\")\n",
    "        else:\n",
    "            response_text = str(observation)\n",
    "        outcome = classifier_func(response_text)\n",
    "        classified.append((\"tool_call\", outcome))\n",
    "    return classified\n",
    "\n",
    "\n",
    "def classify_traces(data, classifier_func, output_path=None):\n",
    "    \"\"\"\n",
    "    Classifies traces in flat data format (one entry per run).\n",
    "    \"\"\"\n",
    "    classified_traces_data = []\n",
    "    \n",
    "    for run in data:  # Flat iteration - each item is a run\n",
    "        trace = run.get(\"trace\", {})\n",
    "        steps = trace.get(\"steps\", [])\n",
    "        \n",
    "        classified_steps = classify_steps(steps, classifier_func)\n",
    "        \n",
    "        entry = {\n",
    "            \"test_suite_name\": run.get(\"test_suite_name\"),\n",
    "            \"test_id\": run[\"test_id\"],\n",
    "            \"test_name\": run.get(\"test_name\"),\n",
    "            \"model\": run[\"model\"],\n",
    "            \"runId\": run.get(\"runId\", run.get(\"run_id\")),\n",
    "            \"passed\": run[\"passed\"],\n",
    "            \"score\": run[\"score\"],\n",
    "            \"time\": run[\"time\"],\n",
    "            \"service\": run.get(\"service\"),\n",
    "            \"trace\": {\n",
    "                **trace,\n",
    "                \"classified_steps\": classified_steps\n",
    "            }\n",
    "        }\n",
    "        classified_traces_data.append(entry)\n",
    "\n",
    "    if output_path:\n",
    "        out_path = Path(output_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(classified_traces_data, f, indent=2)\n",
    "\n",
    "    return classified_traces_data\n",
    "\n",
    "\n",
    "def get_traces_stats(data, classifier_func):\n",
    "    \"\"\"\n",
    "    Compute trace statistics from flat data format.\n",
    "    Requires classifier_func to classify step observations on-the-fly.\n",
    "    \"\"\"\n",
    "    models = set()\n",
    "    traces_stats = {\n",
    "        \"tool_calls_ratio_stats\": defaultdict(list),\n",
    "        \"nonfailed_tool_calls_stats\": defaultdict(list),\n",
    "        \"failed_tool_calls_stats\": defaultdict(list),\n",
    "        \"api_error_stats\": defaultdict(list),\n",
    "        \"tool_formatting_error_stats\": defaultdict(list),\n",
    "        \"server_error_stats\": defaultdict(list),\n",
    "        \"code_error_stats\": defaultdict(list),\n",
    "        \"context_overflow_stats\": defaultdict(list),\n",
    "        \"other_error_stats\": defaultdict(list),\n",
    "    }\n",
    "\n",
    "    # First pass: classify and collect stats\n",
    "    for run in data:\n",
    "        model = run[\"model\"]\n",
    "        models.add(model)\n",
    "        \n",
    "        trace = run.get(\"trace\", {})\n",
    "        steps = trace.get(\"steps\", [])\n",
    "        \n",
    "        if steps:\n",
    "            # Classify each step's observation\n",
    "            tool_outcomes = []\n",
    "            for step in steps:\n",
    "                obs = step.get(\"observation\", {})\n",
    "                if isinstance(obs, dict):\n",
    "                    response_text = obs.get(\"stdout\", \"\") + \" \" + obs.get(\"stderr\", \"\")\n",
    "                else:\n",
    "                    response_text = str(obs)\n",
    "                outcome = classifier_func(response_text)\n",
    "                tool_outcomes.append(outcome)\n",
    "            \n",
    "            if tool_outcomes:\n",
    "                total = len(tool_outcomes)\n",
    "                nonfailed = tool_outcomes.count(\"non-failure\")\n",
    "                api_error = tool_outcomes.count(\"api_error\")\n",
    "                tool_formatting_error = tool_outcomes.count(\"tool_formatting_error\")\n",
    "                server_error = tool_outcomes.count(\"server_error\")\n",
    "                code_error = tool_outcomes.count(\"code_error\")\n",
    "                context_overflow = tool_outcomes.count(\"context_overflow\")\n",
    "                other = tool_outcomes.count(\"other_error\")\n",
    "\n",
    "                failed = server_error + other + api_error + tool_formatting_error + code_error + context_overflow\n",
    "\n",
    "                traces_stats[\"nonfailed_tool_calls_stats\"][model].append(nonfailed / total)\n",
    "                traces_stats[\"server_error_stats\"][model].append(server_error / total)\n",
    "                traces_stats[\"other_error_stats\"][model].append(other / total)\n",
    "                traces_stats[\"api_error_stats\"][model].append(api_error / total)\n",
    "                traces_stats[\"tool_formatting_error_stats\"][model].append(tool_formatting_error / total)\n",
    "                traces_stats[\"code_error_stats\"][model].append(code_error / total)\n",
    "                traces_stats[\"context_overflow_stats\"][model].append(context_overflow / total)\n",
    "                traces_stats[\"failed_tool_calls_stats\"][model].append(failed / total)\n",
    "\n",
    "    # Baseline tool use: minimum tool calls across ALL models for each test_id (successful runs only)\n",
    "    baseline_tool_use_counts = defaultdict(list)\n",
    "\n",
    "    # Group by test for baseline calculation\n",
    "    for run in data:\n",
    "        test_key = (run.get(\"test_suite_name\"), run[\"test_id\"])\n",
    "        if run[\"passed\"]:\n",
    "            trace = run.get(\"trace\", {})\n",
    "            steps = trace.get(\"steps\", [])\n",
    "            tool_count = len(steps)\n",
    "            baseline_tool_use_counts[test_key].append(tool_count)\n",
    "\n",
    "    # Convert to minimum\n",
    "    for test_key in baseline_tool_use_counts:\n",
    "        if baseline_tool_use_counts[test_key]:\n",
    "            baseline_tool_use_counts[test_key] = min(baseline_tool_use_counts[test_key])\n",
    "        else:\n",
    "            baseline_tool_use_counts[test_key] = 0\n",
    "\n",
    "    # Group runs by (model, test_key) for ratio calculation\n",
    "    runs_by_model_test = defaultdict(list)\n",
    "    for run in data:\n",
    "        test_key = (run.get(\"test_suite_name\"), run[\"test_id\"])\n",
    "        model = run[\"model\"]\n",
    "        runs_by_model_test[(model, test_key)].append(run)\n",
    "\n",
    "    # Calculate tool calls ratio\n",
    "    for (model, test_key), runs in runs_by_model_test.items():\n",
    "        if test_key not in baseline_tool_use_counts:\n",
    "            continue\n",
    "\n",
    "        baseline = baseline_tool_use_counts[test_key]\n",
    "\n",
    "        # Collect tool counts for passed runs ONLY\n",
    "        passed_tool_counts = []\n",
    "        for run in runs:\n",
    "            if run[\"passed\"]:\n",
    "                trace = run.get(\"trace\", {})\n",
    "                steps = trace.get(\"steps\", [])\n",
    "                tool_count = len(steps)\n",
    "                passed_tool_counts.append(tool_count)\n",
    "\n",
    "        if passed_tool_counts:\n",
    "            avg_tool_calls_for_test = sum(passed_tool_counts) / len(passed_tool_counts)\n",
    "            if baseline > 0:\n",
    "                ratio = avg_tool_calls_for_test / baseline\n",
    "            else:\n",
    "                ratio = float(avg_tool_calls_for_test) if avg_tool_calls_for_test > 0 else 1.0\n",
    "\n",
    "            traces_stats[\"tool_calls_ratio_stats\"][model].append(ratio)\n",
    "\n",
    "    # Summarize\n",
    "    for model in models:\n",
    "        for category, per_model in traces_stats.items():\n",
    "            row = per_model[model]\n",
    "            if not row:\n",
    "                traces_stats[category][model] = {\"mean\": None, \"median\": None, \"skew\": None, \"std\": None, \"min\": None, \"max\": None}\n",
    "            else:\n",
    "                traces_stats[category][model] = {\n",
    "                    \"mean\": float(np.mean(row)),\n",
    "                    \"median\": float(np.median(row)),\n",
    "                    \"skew\": float(skew(row)) if len(row) >= 3 else None,\n",
    "                    \"std\": float(np.std(row)),\n",
    "                    \"min\": float(np.min(row)),\n",
    "                    \"max\": float(np.max(row)),\n",
    "                }\n",
    "\n",
    "    sorted_traces_stats = {}\n",
    "    for category, model_stats in traces_stats.items():\n",
    "        sorted_traces_stats[category] = dict(\n",
    "            sorted(model_stats.items(), key=lambda item: (item[1][\"mean\"] is None, item[1][\"mean\"]))\n",
    "        )\n",
    "\n",
    "    return sorted_traces_stats\n",
    "\n",
    "\n",
    "def get_recovery_stats(data, classifier_func):\n",
    "    \"\"\"\n",
    "    Compute recovery statistics from flat data format.\n",
    "    \"\"\"\n",
    "    recovery_stats = {\n",
    "        \"recovered_api_error_stats\": defaultdict(list),\n",
    "        \"recovered_tool_formatting_error_stats\": defaultdict(list),\n",
    "        \"recovered_server_error_stats\": defaultdict(list),\n",
    "        \"recovered_code_error_stats\": defaultdict(list),\n",
    "        \"recovered_other_error_stats\": defaultdict(list),\n",
    "\n",
    "        \"recovered_api_error_contribution_stats\": defaultdict(list),\n",
    "        \"recovered_tool_formatting_error_contribution_stats\": defaultdict(list),\n",
    "        \"recovered_server_error_contribution_stats\": defaultdict(list),\n",
    "        \"recovered_code_error_contribution_stats\": defaultdict(list),\n",
    "        \"recovered_other_error_contribution_stats\": defaultdict(list),\n",
    "\n",
    "        \"total_recovery_stats\": defaultdict(list),\n",
    "    }\n",
    "    models = set()\n",
    "    error_types = (\"api_error\", \"tool_formatting_error\", \"server_error\", \"other_error\", \"code_error\")\n",
    "\n",
    "    for run in data:\n",
    "        model = run[\"model\"]\n",
    "        models.add(model)\n",
    "        passed = run[\"passed\"]\n",
    "        \n",
    "        trace = run.get(\"trace\", {})\n",
    "        steps = trace.get(\"steps\", [])\n",
    "\n",
    "        if not steps:\n",
    "            continue\n",
    "\n",
    "        # Classify each step\n",
    "        tool_outcomes = []\n",
    "        for step in steps:\n",
    "            obs = step.get(\"observation\", {})\n",
    "            if isinstance(obs, dict):\n",
    "                response_text = obs.get(\"stdout\", \"\") + \" \" + obs.get(\"stderr\", \"\")\n",
    "            else:\n",
    "                response_text = str(obs)\n",
    "            outcome = classifier_func(response_text)\n",
    "            tool_outcomes.append(outcome)\n",
    "\n",
    "        # Look for: error -> next tool call outcome\n",
    "        for i in range(len(tool_outcomes) - 1):\n",
    "            current_outcome = tool_outcomes[i]\n",
    "            next_outcome = tool_outcomes[i + 1]\n",
    "\n",
    "            if current_outcome in error_types:\n",
    "                recovered = 1 if (passed and next_outcome == \"non-failure\") else 0\n",
    "\n",
    "                recovery_stats[\"total_recovery_stats\"][model].append(recovered)\n",
    "\n",
    "                if current_outcome == \"api_error\":\n",
    "                    recovery_stats[\"recovered_api_error_stats\"][model].append(recovered)\n",
    "                elif current_outcome == \"tool_formatting_error\":\n",
    "                    recovery_stats[\"recovered_tool_formatting_error_stats\"][model].append(recovered)\n",
    "                elif current_outcome == \"server_error\":\n",
    "                    recovery_stats[\"recovered_server_error_stats\"][model].append(recovered)\n",
    "                elif current_outcome == \"code_error\":\n",
    "                    recovery_stats[\"recovered_code_error_stats\"][model].append(recovered)\n",
    "                elif current_outcome == \"other_error\":\n",
    "                    recovery_stats[\"recovered_other_error_stats\"][model].append(recovered)\n",
    "\n",
    "                # Contribution stats\n",
    "                for et in error_types:\n",
    "                    val = 1 if (current_outcome == et and recovered) else 0\n",
    "                    recovery_stats[f\"recovered_{et}_contribution_stats\"][model].append(val)\n",
    "\n",
    "    for model in models:\n",
    "        for category, per_model in recovery_stats.items():\n",
    "            row = per_model[model]\n",
    "            if not row:\n",
    "                recovery_stats[category][model] = {\"mean\": None, \"median\": None, \"skew\": None, \"std\": None, \"min\": None, \"max\": None}\n",
    "            else:\n",
    "                recovery_stats[category][model] = {\n",
    "                    \"mean\": float(np.mean(row)),\n",
    "                    \"median\": float(np.median(row)),\n",
    "                    \"skew\": float(skew(row)) if len(row) >= 3 else None,\n",
    "                    \"std\": float(np.std(row)),\n",
    "                    \"min\": float(np.min(row)),\n",
    "                    \"max\": float(np.max(row)),\n",
    "                }\n",
    "\n",
    "    sorted_recovery_stats = {}\n",
    "    for category, model_stats in recovery_stats.items():\n",
    "        sorted_recovery_stats[category] = dict(\n",
    "            sorted(model_stats.items(), key=lambda item: (item[1][\"mean\"] is None, item[1][\"mean\"]))\n",
    "        )\n",
    "\n",
    "    return sorted_recovery_stats\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_basic_stats(data):\n",
    "    \"\"\"\n",
    "    Compute basic statistics from flat data format.\n",
    "    Returns DataFrames with groupings by model, include_api_docs, and test_suite.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame for easier aggregation\n",
    "    records = []\n",
    "    for run in data:\n",
    "        trace = run.get(\"trace\", {})\n",
    "        usage = trace.get(\"usage\", {})\n",
    "        steps = trace.get(\"steps\", [])\n",
    "        \n",
    "        records.append({\n",
    "            \"model\": run[\"model\"],\n",
    "            \"test_suite_name\": run.get(\"test_suite_name\", \"unknown\"),\n",
    "            \"test_id\": run[\"test_id\"],\n",
    "            \"service\": run.get(\"service\", \"unknown\"),\n",
    "            \"include_api_docs\": run.get(\"include_api_docs\", True),\n",
    "            \"passed\": 1 if run[\"passed\"] else 0,\n",
    "            \"score\": run[\"score\"],\n",
    "            \"time\": run[\"time\"],\n",
    "            \"total_tokens\": usage.get(\"total_tokens\", 0),\n",
    "            \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),\n",
    "            \"completion_tokens\": usage.get(\"completion_tokens\", 0),\n",
    "            \"cost\": usage.get(\"cost\", 0),\n",
    "            \"tool_calls\": len(steps),\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Metrics to aggregate\n",
    "    metrics = [\"passed\", \"score\", \"time\", \"total_tokens\", \"prompt_tokens\", \n",
    "               \"completion_tokens\", \"cost\", \"tool_calls\"]\n",
    "    \n",
    "    def agg_stats(group):\n",
    "        \"\"\"Compute mean, std, count for each metric\"\"\"\n",
    "        result = {\"n_runs\": len(group)}\n",
    "        for m in metrics:\n",
    "            result[f\"{m}_mean\"] = group[m].mean()\n",
    "            result[f\"{m}_std\"] = group[m].std()\n",
    "        return pd.Series(result)\n",
    "    \n",
    "    # 1. Overall stats per model\n",
    "    overall_by_model = df.groupby(\"model\").apply(agg_stats).reset_index()\n",
    "    \n",
    "    # 2. Stats per model + include_api_docs\n",
    "    by_model_docs = df.groupby([\"model\", \"include_api_docs\"]).apply(agg_stats).reset_index()\n",
    "    \n",
    "    # 3. Stats per model + test_suite\n",
    "    by_model_suite = df.groupby([\"model\", \"test_suite_name\"]).apply(agg_stats).reset_index()\n",
    "    \n",
    "    # 4. Stats per model + test_suite + include_api_docs (full breakdown)\n",
    "    by_model_suite_docs = df.groupby([\"model\", \"test_suite_name\", \"include_api_docs\"]).apply(agg_stats).reset_index()\n",
    "    \n",
    "    # 5. Stats per model + service\n",
    "    by_model_service = df.groupby([\"model\", \"service\"]).apply(agg_stats).reset_index()\n",
    "    \n",
    "    return {\n",
    "        \"raw_df\": df,\n",
    "        \"overall_by_model\": overall_by_model,\n",
    "        \"by_model_and_docs\": by_model_docs,\n",
    "        \"by_model_and_suite\": by_model_suite,\n",
    "        \"by_model_suite_docs\": by_model_suite_docs,\n",
    "        \"by_model_and_service\": by_model_service,\n",
    "    }\n",
    "\n",
    "\n",
    "def display_basic_stats(stats, key=\"overall_by_model\", sort_by=\"score_mean\", ascending=False):\n",
    "    \"\"\"\n",
    "    Display a stats table with nice formatting.\n",
    "    \"\"\"\n",
    "    df = stats[key].copy()\n",
    "    \n",
    "    if sort_by in df.columns:\n",
    "        df = df.sort_values(sort_by, ascending=ascending)\n",
    "    \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if \"passed\" in col and \"mean\" in col:\n",
    "            # passed is 0-1, show as percentage\n",
    "            df[col] = df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"-\")\n",
    "        elif \"score\" in col:\n",
    "            # score is 0-100, show as number\n",
    "            df[col] = df[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else \"-\")\n",
    "        elif \"cost\" in col:\n",
    "            df[col] = df[col].apply(lambda x: f\"${x:.4f}\" if pd.notna(x) else \"-\")\n",
    "        elif \"time\" in col:\n",
    "            df[col] = df[col].apply(lambda x: f\"{x:.1f}s\" if pd.notna(x) else \"-\")\n",
    "        else:\n",
    "            df[col] = df[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else \"-\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def print_summary_table(stats):\n",
    "    \"\"\"Print a concise summary table comparing models\"\"\"\n",
    "    df = stats[\"overall_by_model\"].copy()\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"Model\": df[\"model\"],\n",
    "        \"Runs\": df[\"n_runs\"].astype(int),\n",
    "        \"Pass Rate\": df[\"passed_mean\"].apply(lambda x: f\"{x:.1%}\"),  # 0-1 -> percentage\n",
    "        \"Avg Score\": df[\"score_mean\"].apply(lambda x: f\"{x:.1f}\"),   # Already 0-100\n",
    "        \"Avg Time (s)\": df[\"time_mean\"].apply(lambda x: f\"{x:.1f}\"),\n",
    "        \"Avg Cost ($)\": df[\"cost_mean\"].apply(lambda x: f\"{x:.4f}\"),\n",
    "        \"Avg Tool Calls\": df[\"tool_calls_mean\"].apply(lambda x: f\"{x:.1f}\"),\n",
    "    })\n",
    "    \n",
    "    summary = summary.sort_values(\"Avg Score\", ascending=False)\n",
    "    print(\"\\n=== Overall Model Performance ===\")\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # With/without API docs comparison\n",
    "    docs_df = stats[\"by_model_and_docs\"]\n",
    "    print(\"\\n=== Performance by API Docs Inclusion ===\")\n",
    "    docs_summary = pd.DataFrame({\n",
    "        \"Model\": docs_df[\"model\"],\n",
    "        \"API Docs\": docs_df[\"include_api_docs\"].apply(lambda x: \"Yes\" if x else \"No\"),\n",
    "        \"Runs\": docs_df[\"n_runs\"].astype(int),\n",
    "        \"Pass Rate\": docs_df[\"passed_mean\"].apply(lambda x: f\"{x:.1%}\"),\n",
    "        \"Avg Score\": docs_df[\"score_mean\"].apply(lambda x: f\"{x:.1f}\"),  # Fixed\n",
    "    })\n",
    "    docs_summary = docs_summary.sort_values([\"Model\", \"API Docs\"])\n",
    "    print(docs_summary.to_string(index=False))\n",
    "    \n",
    "    # Per suite\n",
    "    suite_df = stats[\"by_model_and_suite\"]\n",
    "    print(\"\\n=== Performance by Test Suite ===\")\n",
    "    suite_summary = pd.DataFrame({\n",
    "        \"Model\": suite_df[\"model\"],\n",
    "        \"Suite\": suite_df[\"test_suite_name\"],\n",
    "        \"Runs\": suite_df[\"n_runs\"].astype(int),\n",
    "        \"Pass Rate\": suite_df[\"passed_mean\"].apply(lambda x: f\"{x:.1%}\"),\n",
    "        \"Avg Score\": suite_df[\"score_mean\"].apply(lambda x: f\"{x:.1f}\"),  # Fixed\n",
    "    })\n",
    "    suite_summary = suite_summary.sort_values([\"Suite\", \"Avg Score\"], ascending=[True, False])\n",
    "    print(suite_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stats\n",
    "stats = get_basic_stats(data)\n",
    "\n",
    "# Print nice summary tables\n",
    "print_summary_table(stats)\n",
    "\n",
    "# Access specific DataFrames\n",
    "stats[\"overall_by_model\"]           # Overall per model\n",
    "stats[\"by_model_and_docs\"]          # Per model + include_api_docs\n",
    "stats[\"by_model_and_suite\"]         # Per model + test suite\n",
    "stats[\"by_model_suite_docs\"]        # Full breakdown\n",
    "stats[\"by_model_and_service\"]       # Per model + service (slack/box/calendar)\n",
    "stats[\"raw_df\"]                     # Raw data as DataFrame\n",
    "\n",
    "# Display formatted table\n",
    "display_basic_stats(stats, key=\"by_model_and_docs\", sort_by=\"score_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa66a6",
   "metadata": {
    "id": "91fa66a6"
   },
   "outputs": [],
   "source": [
    "def plot_grouped_cross_benchmark_bars(slack_stats, linear_stats, metrics_to_plot):\n",
    "    # Find common models across all metrics to ensure consistency\n",
    "    # We'll base it on the first metric, assuming models are consistent across keys\n",
    "    if not metrics_to_plot:\n",
    "        return\n",
    "\n",
    "    base_metric = metrics_to_plot[0]\n",
    "    slack_models = set(slack_stats.get(base_metric, {}).keys())\n",
    "    linear_models = set(linear_stats.get(base_metric, {}).keys())\n",
    "    common_models = sorted(list(slack_models & linear_models))\n",
    "\n",
    "    if not common_models:\n",
    "        print(\"No common models found for comparison.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Comparing {len(common_models)} common models across {len(metrics_to_plot)} metrics.\")\n",
    "\n",
    "    num_metrics = len(metrics_to_plot)\n",
    "    fig, axes = plt.subplots(num_metrics, 1, figsize=(12, 5 * num_metrics))\n",
    "    if num_metrics == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    bar_height = 0.35\n",
    "    y_indices = np.arange(len(common_models))\n",
    "\n",
    "    for ax, metric in zip(axes, metrics_to_plot):\n",
    "        if metric not in slack_stats or metric not in linear_stats:\n",
    "            ax.text(0.5, 0.5, f\"Metric {metric} missing\", ha='center')\n",
    "            continue\n",
    "\n",
    "        # Extract stats\n",
    "        slack_means, slack_stds, slack_skews = [], [], []\n",
    "        linear_means, linear_stds, linear_skews = [], [], []\n",
    "\n",
    "        for m in common_models:\n",
    "            # Slack\n",
    "            s = slack_stats[metric][m]\n",
    "            slack_means.append(s['mean'] if s['mean'] is not None else 0)\n",
    "            slack_stds.append(s['std'] if s['std'] is not None else 0)\n",
    "            slack_skews.append(s['skew'])\n",
    "\n",
    "            # Linear\n",
    "            l = linear_stats[metric][m]\n",
    "            linear_means.append(l['mean'] if l['mean'] is not None else 0)\n",
    "            linear_stds.append(l['std'] if l['std'] is not None else 0)\n",
    "            linear_skews.append(l['skew'])\n",
    "\n",
    "        # Plot Slack bars (Offset up) - NO XERR\n",
    "        ax.barh(y_indices + bar_height/2, slack_means, height=bar_height, label='Slack', color='tab:blue', alpha=0.7)\n",
    "\n",
    "        # Plot Linear bars (Offset down) - NO XERR\n",
    "        ax.barh(y_indices - bar_height/2, linear_means, height=bar_height, label='Linear', color='tab:orange', alpha=0.7)\n",
    "\n",
    "        # Annotate Slack bars\n",
    "        max_val = max(max(slack_means), max(linear_means)) if (slack_means and linear_means) else 1.0\n",
    "\n",
    "        for i, (m, sd, sk) in enumerate(zip(slack_means, slack_stds, slack_skews)):\n",
    "            sk_str = f\"sk={sk:.2f}\" if sk is not None else \"\"\n",
    "            text = f\"σ={sd:.2f} {sk_str}\".strip()\n",
    "            if text:\n",
    "                ax.text(m + 0.01 * max_val, y_indices[i] + bar_height/2, text, va='center', fontsize=8, color='tab:blue')\n",
    "\n",
    "        # Annotate Linear bars\n",
    "        for i, (m, sd, sk) in enumerate(zip(linear_means, linear_stds, linear_skews)):\n",
    "            sk_str = f\"sk={sk:.2f}\" if sk is not None else \"\"\n",
    "            text = f\"σ={sd:.2f} {sk_str}\".strip()\n",
    "            if text:\n",
    "                ax.text(m + 0.01 * max_val, y_indices[i] - bar_height/2, text, va='center', fontsize=8, color='tab:orange')\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_yticks(y_indices)\n",
    "        ax.set_yticklabels(common_models)\n",
    "        ax.set_title(f\"Comparison: {metric.replace('_stats', '').replace('_', ' ').title()}\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "        ax.invert_yaxis()  # Models from top to bottom\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrxoulUoBJYM",
   "metadata": {
    "id": "wrxoulUoBJYM"
   },
   "outputs": [],
   "source": [
    "def get_basic_stats(data):\n",
    "    models = set()\n",
    "    basic_stats = {\n",
    "        \"passed_stats\": defaultdict(list),\n",
    "        \"score_stats\": defaultdict(list),\n",
    "        \"time_stats\": defaultdict(list),\n",
    "        \"total_tokens_stats\": defaultdict(list),\n",
    "        \"prompt_tokens_stats\": defaultdict(list),\n",
    "        \"completion_tokens_stats\": defaultdict(list),\n",
    "        \"total_cost_stats\": defaultdict(list),\n",
    "        \"tool_calls_stats\": defaultdict(list)\n",
    "    }\n",
    "\n",
    "\n",
    "    for test in data:\n",
    "        model = test[\"model\"]\n",
    "        models.add(model)\n",
    "        for run in test[\"runs\"]:\n",
    "            if run[\"passed\"]:\n",
    "                basic_stats[\"passed_stats\"][model].append(1)\n",
    "            else:\n",
    "                basic_stats[\"passed_stats\"][model].append(0)\n",
    "\n",
    "            basic_stats[\"score_stats\"][model].append(run[\"score\"])\n",
    "            basic_stats[\"time_stats\"][model].append(run[\"time\"])\n",
    "            basic_stats[\"total_tokens_stats\"][model].append(run[\"metrics\"][\"total_tokens\"])\n",
    "            basic_stats[\"prompt_tokens_stats\"][model].append(run[\"metrics\"][\"prompt_tokens\"])\n",
    "            basic_stats[\"completion_tokens_stats\"][model].append(run[\"metrics\"][\"completion_tokens\"])\n",
    "            basic_stats[\"total_cost_stats\"][model].append(run[\"metrics\"][\"total_cost\"])\n",
    "            basic_stats[\"tool_calls_stats\"][model].append(run[\"metrics\"][\"tool_calls\"])\n",
    "\n",
    "    for model in models:\n",
    "        for category, data_list in basic_stats.items():\n",
    "            row = data_list[model]\n",
    "            if not row:\n",
    "                basic_stats[category][model] = {\"mean\": None, \"median\": None, \"skew\": None, \"std\": None, \"min\": None, \"max\": None}\n",
    "            else:\n",
    "                basic_stats[category][model] = {\n",
    "                    \"mean\": np.mean(row),\n",
    "                    \"median\": np.median(row),\n",
    "                    \"skew\": skew(row) if len(row) >= 3 else None,\n",
    "                    \"std\": np.std(row),\n",
    "                    \"min\": np.min(row),\n",
    "                    \"max\": np.max(row)\n",
    "                }\n",
    "\n",
    "    sorted_basic_stats = {}\n",
    "    for category, model_stats in basic_stats.items():\n",
    "        sorted_basic_stats[category] = dict(\n",
    "            sorted(\n",
    "                model_stats.items(),\n",
    "                key=lambda item: (item[1][\"mean\"] is None, item[1][\"mean\"])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sorted_basic_stats\n",
    "\n",
    "def plot_scatter_stats(stats_dict, benchmark_name=\"\"):\n",
    "    # We want to plot key metrics against 'score_stats' (quality) and 'passed_stats' (success rate)\n",
    "    y_axis_metrics = ['score_stats', 'passed_stats']\n",
    "    x_axis_metrics = [\n",
    "        'total_cost_stats',\n",
    "        'time_stats',\n",
    "        'total_tokens_stats',\n",
    "        'tool_calls_stats'\n",
    "    ]\n",
    "\n",
    "    # Filter keys that actually exist\n",
    "    y_axis_metrics = [k for k in y_axis_metrics if k in stats_dict]\n",
    "    x_axis_metrics = [k for k in x_axis_metrics if k in stats_dict]\n",
    "\n",
    "    if not y_axis_metrics or not x_axis_metrics:\n",
    "        print(\"Not enough metrics to plot scatters.\")\n",
    "        return\n",
    "\n",
    "    # Flatten plots to a single column\n",
    "    plots_to_make = []\n",
    "    for x_metric in x_axis_metrics:\n",
    "        for y_metric in y_axis_metrics:\n",
    "            plots_to_make.append((x_metric, y_metric))\n",
    "\n",
    "    num_plots = len(plots_to_make)\n",
    "\n",
    "    fig, axes = plt.subplots(num_plots, 1, figsize=(10, 6 * num_plots))\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Get all models\n",
    "    all_models = set()\n",
    "    for k in stats_dict:\n",
    "        all_models.update(stats_dict[k].keys())\n",
    "    models = sorted(list(all_models))\n",
    "\n",
    "    # Generate colors\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(models)))\n",
    "    model_color_map = {m: c for m, c in zip(models, colors)}\n",
    "\n",
    "    for ax, (x_metric, y_metric) in zip(axes, plots_to_make):\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        curr_colors = []\n",
    "        curr_models = []\n",
    "\n",
    "        for model in models:\n",
    "            if model in stats_dict[x_metric] and model in stats_dict[y_metric]:\n",
    "                x_mean = stats_dict[x_metric][model]['mean']\n",
    "                y_mean = stats_dict[y_metric][model]['mean']\n",
    "\n",
    "                if x_mean is not None and y_mean is not None:\n",
    "                    x_vals.append(x_mean)\n",
    "                    y_vals.append(y_mean)\n",
    "                    curr_colors.append(model_color_map[model])\n",
    "                    curr_models.append(model)\n",
    "\n",
    "        if x_vals:\n",
    "            scatter = ax.scatter(x_vals, y_vals, c=curr_colors, s=100, alpha=0.7, edgecolors='k')\n",
    "\n",
    "            # Annotate points\n",
    "            for i, txt in enumerate(curr_models):\n",
    "                ax.annotate(txt, (x_vals[i], y_vals[i]), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "        x_label = x_metric.replace('_stats', '').replace('_', ' ').title()\n",
    "        y_label = y_metric.replace('_stats', '').replace('_', ' ').title()\n",
    "\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_title(f\"{benchmark_name}: {y_label} vs {x_label}\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cross_benchmark_stats(slack_stats, linear_stats):\n",
    "    # Compare Slack vs Linear for common models\n",
    "    metrics_to_compare = [\n",
    "        'passed_stats',\n",
    "        'score_stats',\n",
    "        'total_cost_stats',\n",
    "        'time_stats'\n",
    "    ]\n",
    "\n",
    "    # Find common models\n",
    "    slack_models = set(slack_stats['passed_stats'].keys())\n",
    "    linear_models = set(linear_stats['passed_stats'].keys())\n",
    "    common_models = sorted(list(slack_models & linear_models))\n",
    "\n",
    "    if not common_models:\n",
    "        print(\"No common models found between Slack and Linear benchmarks.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Comparing {len(common_models)} common models: {', '.join(common_models)}\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(metrics_to_compare), 1, figsize=(10, 6 * len(metrics_to_compare)))\n",
    "    if len(metrics_to_compare) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(common_models)))\n",
    "    model_color_map = {m: c for m, c in zip(common_models, colors)}\n",
    "\n",
    "    for ax, metric in zip(axes, metrics_to_compare):\n",
    "        if metric not in slack_stats or metric not in linear_stats:\n",
    "            continue\n",
    "\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        curr_colors = []\n",
    "        curr_models = []\n",
    "\n",
    "        for model in common_models:\n",
    "            slack_val = slack_stats[metric][model]['mean']\n",
    "            linear_val = linear_stats[metric][model]['mean']\n",
    "\n",
    "            if slack_val is not None and linear_val is not None:\n",
    "                x_vals.append(linear_val)\n",
    "                y_vals.append(slack_val)\n",
    "                curr_colors.append(model_color_map[model])\n",
    "                curr_models.append(model)\n",
    "\n",
    "        if x_vals:\n",
    "            ax.scatter(x_vals, y_vals, c=curr_colors, s=120, alpha=0.8, edgecolors='k')\n",
    "\n",
    "            # Draw y=x line for reference\n",
    "            min_val = min(min(x_vals), min(y_vals))\n",
    "            max_val = max(max(x_vals), max(y_vals))\n",
    "            padding = (max_val - min_val) * 0.1\n",
    "            ax.plot([min_val-padding, max_val+padding], [min_val-padding, max_val+padding],\n",
    "                    'r--', alpha=0.3, label='Match (x=y)')\n",
    "\n",
    "            # Annotate\n",
    "            for i, txt in enumerate(curr_models):\n",
    "                ax.annotate(txt, (x_vals[i], y_vals[i]), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "            metric_name = metric.replace('_stats', '').replace('_', ' ').title()\n",
    "            ax.set_xlabel(f\"Linear {metric_name}\")\n",
    "            ax.set_ylabel(f\"Slack {metric_name}\")\n",
    "            ax.set_title(f\"Cross-Benchmark Comparison: {metric_name}\")\n",
    "            ax.grid(True, linestyle='--', alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "py_Q0VOIAoed",
   "metadata": {
    "id": "py_Q0VOIAoed"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stats(stats_dict):\n",
    "    num_categories = len(stats_dict)\n",
    "    fig, axes = plt.subplots(num_categories, 1, figsize=(12, 4 * num_categories))\n",
    "\n",
    "    if num_categories == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (category, model_stats) in zip(axes, stats_dict.items()):\n",
    "        # Filter out models with None mean\n",
    "        valid_models = {k: v for k, v in model_stats.items() if v[\"mean\"] is not None}\n",
    "\n",
    "        if not valid_models:\n",
    "            ax.set_title(f\"{category}: No data\")\n",
    "            continue\n",
    "\n",
    "        models = list(valid_models.keys())\n",
    "        means = [valid_models[m][\"mean\"] for m in models]\n",
    "        stds = [valid_models[m][\"std\"] or 0 for m in models]\n",
    "        skews = [valid_models[m][\"skew\"] for m in models]\n",
    "\n",
    "        # Sort by mean\n",
    "        sorted_idx = sorted(range(len(means)), key=lambda i: means[i])\n",
    "        models = [models[i] for i in sorted_idx]\n",
    "        means = [means[i] for i in sorted_idx]\n",
    "        stds = [stds[i] for i in sorted_idx]\n",
    "        skews = [skews[i] for i in sorted_idx]\n",
    "\n",
    "        y_pos = range(len(models))\n",
    "        ax.barh(y_pos, means, xerr=stds, capsize=3, alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(models)\n",
    "        ax.set_xlabel(category)\n",
    "        ax.set_title(f\"{category}: mean ± std (skew annotated)\")\n",
    "\n",
    "        # Annotate skew\n",
    "        for i, (mean_val, std_val) in enumerate(zip(means, stds)):\n",
    "          ax.text(mean_val + std_val + 0.02 * max_mean, i, f\"σ={std_val:.2f}\",va=\"center\", ha=\"left\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHCbUDZwUjZk",
   "metadata": {
    "id": "nHCbUDZwUjZk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define consistent colors for error types\n",
    "ERROR_COLOR_MAP = {\n",
    "    \"api_error\": \"tab:red\",\n",
    "    \"tool_formatting_error\": \"tab:orange\",\n",
    "    \"server_error\": \"tab:gray\",\n",
    "    \"code_error\": \"tab:blue\",\n",
    "    \"context_overflow\": \"tab:purple\",\n",
    "    \"other_error\": \"tab:green\"\n",
    "}\n",
    "\n",
    "def plot_stats(stats_dict):\n",
    "    # Determine all unique models across all categories to ensure consistent ordering\n",
    "    all_models = set()\n",
    "    for category_data in stats_dict.values():\n",
    "        all_models.update(category_data.keys())\n",
    "    # Sort models for consistent plotting order\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Define categories that should be grouped into stacked bars and their components\n",
    "    # The key here will be the new plot title for the stacked chart\n",
    "    stacked_category_definitions = {\n",
    "        \"Failed Tool Calls Breakdown\": [\n",
    "            \"api_error_stats\",\n",
    "            \"tool_formatting_error_stats\",\n",
    "            \"server_error_stats\",\n",
    "            \"code_error_stats\",\n",
    "            \"context_overflow_stats\",\n",
    "            \"other_error_stats\"\n",
    "        ],\n",
    "        \"Recovered Errors Breakdown (Contribution to Total Recovery Rate)\": [\n",
    "            \"recovered_api_error_contribution_stats\",\n",
    "            \"recovered_tool_formatting_error_contribution_stats\",\n",
    "            \"recovered_server_error_contribution_stats\",\n",
    "            \"recovered_code_error_contribution_stats\",\n",
    "            \"recovered_other_error_contribution_stats\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify categories that are components of a stacked chart, so they are not plotted individually\n",
    "    components_to_skip_individual_plot = set()\n",
    "    for group_components in stacked_category_definitions.values():\n",
    "        components_to_skip_individual_plot.update(group_components)\n",
    "    # Also skip the original 'failed_tool_calls_stats' as we're showing its breakdown\n",
    "    components_to_skip_individual_plot.add(\"failed_tool_calls_stats\")\n",
    "    # Skip the raw recovery rates too, as we are plotting the contribution breakdown\n",
    "    components_to_skip_individual_plot.update([\n",
    "        \"recovered_api_error_stats\", \"recovered_tool_formatting_error_stats\",\n",
    "        \"recovered_server_error_stats\", \"recovered_code_error_stats\", \"recovered_other_error_stats\"\n",
    "    ])\n",
    "\n",
    "    # Prepare a list of categories (or group names for stacked charts) to plot\n",
    "    plots_to_generate = []\n",
    "\n",
    "    # Add individual non-stacked categories first\n",
    "    for category in stats_dict.keys():\n",
    "        if category not in components_to_skip_individual_plot:\n",
    "            # Check if this category name was also a \"group_name\". If so, we'll plot it as a group.\n",
    "            if category not in stacked_category_definitions:\n",
    "                plots_to_generate.append({\"type\": \"individual\", \"name\": category})\n",
    "\n",
    "    # Add explicit stacked groups\n",
    "    for group_name in stacked_category_definitions:\n",
    "        # Check if any component data exists in stats_dict for this group\n",
    "        if any(comp in stats_dict for comp in stacked_category_definitions[group_name]):\n",
    "            plots_to_generate.append({\"type\": \"stacked\", \"name\": group_name})\n",
    "\n",
    "\n",
    "    num_plots = len(plots_to_generate)\n",
    "    if num_plots == 0:\n",
    "        print(\"No categories to plot.\")\n",
    "        return\n",
    "\n",
    "    # Adjust figure size dynamically based on number of plots and models\n",
    "    fig, axes = plt.subplots(num_plots, 1, figsize=(12, 0.5 * len(all_models) * num_plots + 3), constrained_layout=True)\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    plot_idx = 0\n",
    "    for plot_info in plots_to_generate:\n",
    "        category_name = plot_info[\"name\"]\n",
    "        plot_type = plot_info[\"type\"]\n",
    "        ax = axes[plot_idx]\n",
    "\n",
    "        if plot_type == \"stacked\":\n",
    "            components = stacked_category_definitions[category_name]\n",
    "\n",
    "            # Aggregate data for stacked plot\n",
    "            model_values = defaultdict(lambda: {comp: 0.0 for comp in components})\n",
    "            for comp_key in components:\n",
    "                if comp_key in stats_dict: # Ensure the component exists in the provided stats\n",
    "                    for model, stats in stats_dict[comp_key].items():\n",
    "                        if stats[\"mean\"] is not None:\n",
    "                            model_values[model][comp_key] = stats[\"mean\"]\n",
    "\n",
    "            # Filter and sort models based on total mean of components\n",
    "            plottable_models_data = []\n",
    "            for model in all_models:\n",
    "                total_mean_for_model = sum(model_values[model][comp] for comp in components)\n",
    "                # Only include models if they have any non-zero data for the stacked components\n",
    "                if total_mean_for_model > 0 or any(model_values[model][comp] > 0 for comp in components):\n",
    "                    plottable_models_data.append((model, total_mean_for_model, {comp: model_values[model][comp] for comp in components}))\n",
    "\n",
    "            if not plottable_models_data:\n",
    "                ax.set_title(f\"{category_name}: No data\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "\n",
    "            # Sort by total mean (ascending)\n",
    "            plottable_models_data.sort(key=lambda x: x[1])\n",
    "\n",
    "            sorted_models = [data[0] for data in plottable_models_data]\n",
    "            component_data_for_plot = {comp: [data[2][comp] for data in plottable_models_data] for comp in components}\n",
    "\n",
    "            y_pos = np.arange(len(sorted_models))\n",
    "            bottom = np.zeros(len(sorted_models))\n",
    "\n",
    "            handles = []\n",
    "            labels = []\n",
    "\n",
    "            for i, comp_key in enumerate(components):\n",
    "                if comp_key in component_data_for_plot:\n",
    "                    # Determine color based on error type key\n",
    "                    # Strip suffixes to find the base error type\n",
    "                    base_key = comp_key.replace('_stats', '').replace('recovered_', '').replace('_contribution', '')\n",
    "                    color = ERROR_COLOR_MAP.get(base_key, 'gray') # Default to gray if not found\n",
    "\n",
    "                    bars = ax.barh(y_pos, component_data_for_plot[comp_key], left=bottom,\n",
    "                                   label=base_key,\n",
    "                                   color=color)\n",
    "                    bottom += np.array(component_data_for_plot[comp_key])\n",
    "                    if bars:\n",
    "                        handles.append(bars[0])\n",
    "                        labels.append(base_key)\n",
    "\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(sorted_models)\n",
    "            ax.set_xlabel(\"Proportion\")\n",
    "            ax.set_title(category_name)\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "            if handles:\n",
    "                ax.legend(handles=handles, labels=labels, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "        else: # Individual bar chart\n",
    "            # Existing logic for individual bar charts\n",
    "            model_stats = stats_dict[category_name]\n",
    "\n",
    "            valid_models = {k: v for k, v in model_stats.items() if v[\"mean\"] is not None}\n",
    "\n",
    "            if not valid_models:\n",
    "                ax.set_title(f\"{category_name}: No data\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "\n",
    "            models = list(valid_models.keys())\n",
    "            means = [valid_models[m][\"mean\"] for m in models]\n",
    "            stds = [valid_models[m][\"std\"] or 0 for m in models]\n",
    "            skews = [valid_models[m][\"skew\"] for m in models]\n",
    "\n",
    "            # Sort by mean (ascending)\n",
    "            sorted_idx = sorted(range(len(means)), key=lambda i: means[i])\n",
    "            models = [models[i] for i in sorted_idx]\n",
    "            means = [means[i] for i in sorted_idx]\n",
    "            stds = [stds[i] for i in sorted_idx]\n",
    "            skews = [skews[i] for i in sorted_idx]\n",
    "\n",
    "            y_pos = np.arange(len(models))\n",
    "            ax.barh(y_pos, means, xerr=stds, capsize=3, alpha=0.7)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(models)\n",
    "            ax.set_xlabel(category_name)\n",
    "            ax.set_title(f\"{category_name}: mean \\u00B1 std (skew annotated)\")\n",
    "\n",
    "            # Annotate skew and std\n",
    "            max_mean = max(means) if means else 1.0\n",
    "            for i, (mean_val, skew_val, std_val) in enumerate(zip(means, skews, stds)):\n",
    "                sk_text = \"\"\n",
    "                if skew_val is not None and not np.isnan(skew_val):\n",
    "                    sk_text = f\"skew={skew_val:.2f}\"\n",
    "                std_text = f\"std={std_val:.2f}\"\n",
    "\n",
    "                annotation_text = f\"{std_text} {sk_text}\".strip()\n",
    "\n",
    "                ax.text(\n",
    "                    mean_val + std_val + 0.02 * max_mean,\n",
    "                    i,\n",
    "                    annotation_text,\n",
    "                    va=\"center\",\n",
    "                    ha=\"left\"\n",
    "                )\n",
    "\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2l_rkPvlMpz",
   "metadata": {
    "id": "s2l_rkPvlMpz"
   },
   "source": [
    "# Analyze Slack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sC7afDd672bm",
   "metadata": {
    "id": "sC7afDd672bm"
   },
   "source": [
    "## Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j7BNbTllAvSf",
   "metadata": {
    "id": "j7BNbTllAvSf"
   },
   "outputs": [],
   "source": [
    "slack_path = Path(\"/content/drive/MyDrive/API research/evaluation runs/Slack/normalized/full_results_20260115_180540.json_normalized.json\")\n",
    "with slack_path.open() as f:\n",
    "    slack_data_raw = json.load(f)\n",
    "\n",
    "slack_out_path = \"/content/drive/MyDrive/API research/evaluation runs/Slack/normalized/slack_classified_traces_180540.json\"\n",
    "slack_traces_data = classify_traces(slack_data_raw, classify_tool_response_slack, slack_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFLZ4ZPzxmit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6412,
     "status": "ok",
     "timestamp": 1768571175311,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "SFLZ4ZPzxmit",
    "outputId": "5eec54bb-0277-4bae-cb1c-f55bd1315bed"
   },
   "outputs": [],
   "source": [
    "slack_basic_stats = get_basic_stats(slack_data_raw)\n",
    "plot_stats(slack_basic_stats)\n",
    "plot_scatter_stats(slack_basic_stats, \"Slack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqMpzja9NEhd",
   "metadata": {
    "id": "pqMpzja9NEhd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOcahqf88uSa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1768571177341,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "XOcahqf88uSa",
    "outputId": "6c26c262-70ee-46a2-8ea8-d13880be6bee"
   },
   "outputs": [],
   "source": [
    "slack_traces_stats = get_traces_stats(slack_traces_data)\n",
    "plot_stats(slack_traces_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4Xc0rFlFxwnC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1005,
     "status": "ok",
     "timestamp": 1768571178348,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "4Xc0rFlFxwnC",
    "outputId": "cd363e78-c3ac-4bb6-8a6f-ca548efcb858"
   },
   "outputs": [],
   "source": [
    "slack_recovery_stats = get_recovery_stats(slack_traces_data)\n",
    "plot_stats(slack_recovery_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpO5wKzJA33V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2626,
     "status": "ok",
     "timestamp": 1768571180976,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "NpO5wKzJA33V",
    "outputId": "351113ee-38f7-4a61-8257-a7aa242b2829"
   },
   "outputs": [],
   "source": [
    "all_slack_stats = {**slack_traces_stats, **slack_recovery_stats}\n",
    "plot_stats(all_slack_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c802a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1768571181854,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "08c802a4",
    "outputId": "8be4eede-4a06-4868-b7da-57cdebd32426"
   },
   "outputs": [],
   "source": [
    "def inspect_other_errors(data, classifier_func, limit=20):\n",
    "    count = 0\n",
    "    print(f\"--- Inspecting 'other_error' classifications (Showing first {limit}) ---\")\n",
    "    for test in data:\n",
    "        model = test[\"model\"]\n",
    "        test_id = test[\"test_id\"]\n",
    "        for run in test[\"runs\"]:\n",
    "            if \"messages\" not in run[\"trace\"]:\n",
    "                continue\n",
    "\n",
    "            # Build response map first to match tools to responses\n",
    "            response_map = {}\n",
    "            messages = run[\"trace\"][\"messages\"]\n",
    "            for message in messages:\n",
    "                if \"tool_call_id\" in message:\n",
    "                    response_map[message[\"tool_call_id\"]] = message.get(\"content\", \"\")\n",
    "\n",
    "            # Check tool calls\n",
    "            for message in messages:\n",
    "                if \"tool_calls\" in message:\n",
    "                    for tool_call in message[\"tool_calls\"]:\n",
    "                        tool_call_id = tool_call[\"id\"]\n",
    "                        if tool_call_id in response_map:\n",
    "                            content = response_map[tool_call_id]\n",
    "                            label = classifier_func(content)\n",
    "                            if label == \"other_error\":\n",
    "                                print(f\"\\n[Model: {model}] [Test: {test_id}] [Run: {run['run_id']}]\")\n",
    "\n",
    "                                # Safe access for tool name\n",
    "                                if 'function' in tool_call:\n",
    "                                    tool_name = tool_call['function'].get('name', 'Unknown Function')\n",
    "                                else:\n",
    "                                    tool_name = f\"Unknown Tool Structure (keys: {list(tool_call.keys())})\"\n",
    "\n",
    "                                print(f\"Tool Name: {tool_name}\")\n",
    "                                print(f\"Error Content snippet:\\n{content[:600]}...\" if len(content) > 600 else f\"Error Content:\\n{content}\")\n",
    "                                print(\"-\" * 40)\n",
    "                                count += 1\n",
    "                                if count >= limit:\n",
    "                                    return\n",
    "    if count == 0:\n",
    "        print(\"No 'other_error' found.\")\n",
    "\n",
    "# Run the inspection on the raw Slack data\n",
    "inspect_other_errors(slack_data_raw, classify_tool_response_slack, limit=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead8244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1768571185678,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "6ead8244",
    "outputId": "00430b4d-b5fc-46e9-fea6-622d4ec43f19"
   },
   "outputs": [],
   "source": [
    "def collect_and_save_error_samples(data, classifier_func, output_path):\n",
    "    # Structure: samples[model][error_type] = [ { test_id, run_id, tool_name, content }, ... ]\n",
    "    samples = defaultdict(lambda: defaultdict(list))\n",
    "    limit_per_type = 10\n",
    "\n",
    "    print(f\"--- Collecting up to {limit_per_type} samples per error type per model ---\")\n",
    "\n",
    "    for test in data:\n",
    "        model = test[\"model\"]\n",
    "        test_id = test[\"test_id\"]\n",
    "        for run in test[\"runs\"]:\n",
    "            if \"messages\" not in run[\"trace\"]:\n",
    "                continue\n",
    "\n",
    "            # Build response map\n",
    "            response_map = {}\n",
    "            messages = run[\"trace\"][\"messages\"]\n",
    "            for message in messages:\n",
    "                if \"tool_call_id\" in message:\n",
    "                    response_map[message[\"tool_call_id\"]] = message.get(\"content\", \"\")\n",
    "\n",
    "            # Check tool calls\n",
    "            for message in messages:\n",
    "                if \"tool_calls\" in message:\n",
    "                    for tool_call in message[\"tool_calls\"]:\n",
    "                        tool_call_id = tool_call[\"id\"]\n",
    "                        if tool_call_id in response_map:\n",
    "                            content = response_map[tool_call_id]\n",
    "                            label = classifier_func(content)\n",
    "\n",
    "                            # We only care about errors, not successes or 'no_response'\n",
    "                            if label != \"non-failure\" and label != \"no_response\":\n",
    "\n",
    "                                # Check if we already have enough samples for this model+error_type\n",
    "                                if len(samples[model][label]) < limit_per_type:\n",
    "\n",
    "                                    # Safe tool name extraction\n",
    "                                    if 'function' in tool_call:\n",
    "                                        tool_name = tool_call['function'].get('name', 'Unknown Function')\n",
    "                                    else:\n",
    "                                        tool_name = f\"Unknown Structure keys: {list(tool_call.keys())}\"\n",
    "\n",
    "                                    entry = {\n",
    "                                        \"test_id\": test_id,\n",
    "                                        \"run_id\": run[\"run_id\"],\n",
    "                                        \"tool_name\": tool_name,\n",
    "                                        \"error_type\": label,\n",
    "                                        \"content\": content\n",
    "                                    }\n",
    "                                    samples[model][label].append(entry)\n",
    "\n",
    "    # Save to file\n",
    "    out_path = Path(output_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Convert defaultdict to regular dict for JSON serialization\n",
    "    final_output = {k: dict(v) for k, v in samples.items()}\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "\n",
    "    print(f\"Samples saved to: {out_path}\")\n",
    "\n",
    "    # Print a preview (1 of each type found for each model)\n",
    "    print(\"\\n--- Preview of Collected Errors ---\")\n",
    "    for model, error_types in samples.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        for err_type, examples in error_types.items():\n",
    "            ex = examples[0]\n",
    "            print(f\"  [{err_type}] Tool: {ex['tool_name']}\")\n",
    "            preview = ex['content'][:200].replace('\\n', ' ') + \"...\" if len(ex['content']) > 100 else ex['content'].replace('\\n', ' ')\n",
    "            print(f\"    -> {preview}\")\n",
    "\n",
    "# Run the collection\n",
    "output_file = \"/content/drive/MyDrive/API research/evaluation runs/Slack/normalized/slack_error_samples.json\"\n",
    "collect_and_save_error_samples(slack_data_raw, classify_tool_response_slack, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uyAaBEdLLYmX",
   "metadata": {
    "id": "uyAaBEdLLYmX"
   },
   "source": [
    "# Analyze Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m6gB0RJyLYVB",
   "metadata": {
    "id": "m6gB0RJyLYVB"
   },
   "outputs": [],
   "source": [
    "linear_path = Path(\"/content/drive/MyDrive/API research/evaluation runs/Linear/normalized/linear_full_results_normalized.json\")\n",
    "with linear_path.open() as f:\n",
    "    linear_data_raw = json.load(f)\n",
    "\n",
    "linear_out_path = \"/content/drive/MyDrive/API research/evaluation runs/Linear/normalized/linear_classified.json\"\n",
    "linear_traces_data = classify_traces(linear_data_raw, classify_tool_response_linear, linear_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qf4SQr8fLyrN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5916,
     "status": "ok",
     "timestamp": 1768571224190,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "Qf4SQr8fLyrN",
    "outputId": "cb83f34d-a9ff-40e6-d811-4fa8e64f9643"
   },
   "outputs": [],
   "source": [
    "linear_basic_stats = get_basic_stats(linear_data_raw)\n",
    "plot_stats(linear_basic_stats)\n",
    "plot_scatter_stats(linear_basic_stats, \"Linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PN_ypkk0M78x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1768571228703,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "PN_ypkk0M78x",
    "outputId": "5ab95ef7-773b-4985-f2ca-33851d49c51c"
   },
   "outputs": [],
   "source": [
    "linear_traces_stats = get_traces_stats(linear_traces_data)\n",
    "plot_stats(slack_traces_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asdGE5DDMqPj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "ok",
     "timestamp": 1768571231924,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "asdGE5DDMqPj",
    "outputId": "aca7ab90-115c-4870-d474-a82c77b2fbe1"
   },
   "outputs": [],
   "source": [
    "linear_recovery_stats = get_recovery_stats(linear_traces_data)\n",
    "plot_stats(linear_recovery_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WlzVr2rMnRu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2586,
     "status": "ok",
     "timestamp": 1768571236865,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "5WlzVr2rMnRu",
    "outputId": "e8418c62-6b5a-495f-eb08-76bebeb58edd"
   },
   "outputs": [],
   "source": [
    "all_linear_stats = {**linear_traces_stats, **linear_recovery_stats}\n",
    "plot_stats(all_linear_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd70f65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1768571244436,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "7bd70f65",
    "outputId": "9ac5a68a-8311-4fbe-f12c-76f04115b307"
   },
   "outputs": [],
   "source": [
    "plot_cross_benchmark_stats(slack_basic_stats, linear_basic_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_grouped_comparison",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4853,
     "status": "ok",
     "timestamp": 1768571281952,
     "user": {
      "displayName": "Artem Zhuravel",
      "userId": "03530997300349037171"
     },
     "user_tz": -330
    },
    "id": "run_grouped_comparison",
    "outputId": "bd6c7a8d-03a6-4820-bccc-5966653491ba"
   },
   "outputs": [],
   "source": [
    "# Prepare combined stats dictionaries\n",
    "# Merging basic stats + trace stats (errors) + recovery stats\n",
    "full_slack_stats = {**slack_basic_stats, **slack_traces_stats, **slack_recovery_stats}\n",
    "full_linear_stats = {**linear_basic_stats, **linear_traces_stats, **linear_recovery_stats}\n",
    "\n",
    "# Define metrics we want to compare\n",
    "metrics_of_interest = [\n",
    "    'passed_stats',           # Success Rate\n",
    "    'score_stats',            # Quality Score\n",
    "    'time_stats',             # Execution Time\n",
    "    'total_tokens_stats',     # Token Usage\n",
    "    'failed_tool_calls_stats',# Overall Error Rate\n",
    "    'api_error_stats',        # Specific Error: API\n",
    "    'code_error_stats',       # Specific Error: Code\n",
    "    'tool_formatting_error_stats', # Specific Error: Formatting\n",
    "    'total_recovery_stats'\n",
    "]\n",
    "\n",
    "# Re-run plotting with annotations\n",
    "plot_grouped_cross_benchmark_bars(full_slack_stats, full_linear_stats, metrics_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZlQqrZ1x5wbm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "error",
     "timestamp": 1769155886675,
     "user": {
      "displayName": "Hubert Pyskło",
      "userId": "16424235253540376255"
     },
     "user_tz": -330
    },
    "id": "ZlQqrZ1x5wbm",
    "outputId": "f13da6b4-bd34-4e57-c57d-5783cc2b87fa"
   },
   "outputs": [],
   "source": [
    "def plot_cost_vs_score_combined(slack_stats, linear_stats):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Helper to extract data\n",
    "    def extract_data(stats, cost_key='total_cost_stats', score_key='score_stats'):\n",
    "        costs = []\n",
    "        scores = []\n",
    "        labels = []\n",
    "        for model, metric_stats in stats[cost_key].items():\n",
    "            cost_val = metric_stats['mean']\n",
    "            score_val = stats[score_key][model]['mean']\n",
    "\n",
    "            if cost_val is not None and score_val is not None:\n",
    "                costs.append(cost_val)\n",
    "                scores.append(score_val)\n",
    "                labels.append(model)\n",
    "        return costs, scores, labels\n",
    "\n",
    "    # Slack Data\n",
    "    s_costs, s_scores, s_labels = extract_data(slack_stats)\n",
    "    # Linear Data\n",
    "    l_costs, l_scores, l_labels = extract_data(linear_stats)\n",
    "\n",
    "    # Plot\n",
    "    ax.scatter(s_costs, s_scores, c='tab:blue', label='Slack', s=100, alpha=0.7, edgecolors='k', marker='o')\n",
    "    ax.scatter(l_costs, l_scores, c='tab:orange', label='Linear', s=100, alpha=0.7, edgecolors='k', marker='^')\n",
    "\n",
    "    # Annotate Slack\n",
    "    for i, label in enumerate(s_labels):\n",
    "        ax.annotate(label, (s_costs[i], s_scores[i]), xytext=(5, 5), textcoords='offset points', fontsize=8, color='tab:blue', alpha=0.8)\n",
    "\n",
    "    # Annotate Linear\n",
    "    for i, label in enumerate(l_labels):\n",
    "        ax.annotate(label, (l_costs[i], l_scores[i]), xytext=(5, -10), textcoords='offset points', fontsize=8, color='tab:orange', alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Mean Total Cost ($)\")\n",
    "    ax.set_ylabel(\"Mean Score\")\n",
    "    ax.set_title(\"Cost vs. Score: Slack vs Linear\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the stats computed in previous cells\n",
    "plot_cost_vs_score_combined(slack_basic_stats, linear_basic_stats)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
